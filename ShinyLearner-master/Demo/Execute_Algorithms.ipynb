{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example classification analysis using ShinyLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*By Erica Suh and Stephen Piccolo*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to perform a benchmark comparison of classification algorithms using [ShinyLearner](https://github.com/srp33/ShinyLearner). We assume the reader has a moderate level of understanding of shell and Python scripting. We also assume that the user's operating system is UNIX-based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# This step may or may not be necessary on your system:\n",
    "pip3 install --upgrade pip\n",
    "\n",
    "# You only need to install these modules once\n",
    "pip3 install pmlb pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate a \"null\" dataset that contains no signal to ensure that ShinyLearner doesn't find a signal when there is nothing to be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import shutil\n",
    "\n",
    "def one_hot_encode(file_path, column_names):\n",
    "    data = pandas.read_csv(file_path, index_col=0, sep=\"\\t\")\n",
    "    \n",
    "    if column_names == None:\n",
    "        column_names = [x for x in list(data) if not x in [\"Class\"]]\n",
    "\n",
    "    data = pandas.get_dummies(data, drop_first=True, columns=column_names)\n",
    "    data.to_csv(file_path, sep=\"\\t\", index=True)\n",
    "    \n",
    "directory = \"Datasets\"\n",
    "if os.path.exists(directory):\n",
    "    shutil.rmtree(directory)\n",
    "os.makedirs(directory)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "num_observations = 500\n",
    "num_numeric_features = 20\n",
    "num_discrete_features = 10\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "data_dict[\"\"] = [\"Instance{}\".format(i+1) for i in range(num_observations)]\n",
    "data_dict[\"Class\"] = np.random.choice([0, 1], size=num_observations, p=[0.5, 0.5])\n",
    "\n",
    "for i in range(num_numeric_features):\n",
    "    data_dict[\"Numeric{}\".format(i+1)] = np.random.normal(0, 1, num_observations)\n",
    "for i in range(num_discrete_features):\n",
    "    data_dict[\"Discrete{}\".format(i+1)] = np.random.choice([\"A\", \"B\", \"C\"], size=num_observations, p=[0.4, 0.5, 0.1])\n",
    "\n",
    "df = pandas.DataFrame(data=data_dict)\n",
    "df.set_index(\"\", inplace=True)\n",
    "\n",
    "file_path = '{}/{}.tsv'.format(directory, \"null\")\n",
    "\n",
    "df.to_csv(file_path, sep=\"\\t\", index=True)\n",
    "one_hot_encode('{}/{}.tsv'.format(directory, \"null\"), [x for x in data_dict.keys() if x.startswith(\"Discrete\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Penn Machine Learning Benchmarks (PMLB) repository contains a large number of datasets that can be used to test machine-learning algorithms. We can access this repository using the Python module named `pmlb`. For demonstration purposes, we will fetch 10 biology-related datasets from PMLB. First, define a list that indicates the unique identifier for each of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['analcatdata_aids',\n",
    "            'ann-thyroid',\n",
    "            'breast-cancer',\n",
    "            'dermatology',\n",
    "            'diabetes',\n",
    "            'hepatitis',\n",
    "            'iris',\n",
    "            'liver-disorder',\n",
    "            'molecular-biology_promoters',\n",
    "            'yeast']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShinyLearner requires that input data files have exactly one feature named 'Class', which includes the class labels. So we must modify the PMLB data to meet this requirement. After modifying the data, we save each DataFrame to a a file with a [supported extension](https://github.com/srp33/ShinyLearner/blob/master/InputFormats.md). (See PMLB's [GitHub repository](https://github.com/EpistasisLab/penn-ml-benchmarks) for more information about how to use this module.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmlb import fetch_data\n",
    "\n",
    "for data in datasets:\n",
    "    curr_data = fetch_data(data)\n",
    "    curr_data = curr_data.rename(columns={'target': 'Class'})  # Rename 'target' to 'Class'\n",
    "    \n",
    "    if data == \"molecular-biology_promoters\":\n",
    "        curr_data = curr_data.drop(columns=[\"instance\"], axis=1)\n",
    "    \n",
    "    curr_data.to_csv('{}/{}.tsv'.format(directory, data), sep='\\t', index=True)  # Save to a .tsv file\n",
    "\n",
    "one_hot_encode('{}/{}.tsv'.format(directory, \"analcatdata_aids\"), [\"Race\"])\n",
    "one_hot_encode('{}/{}.tsv'.format(directory, \"breast-cancer\"), [\"menopause\", \"breast-quad\"])\n",
    "one_hot_encode('{}/{}.tsv'.format(directory, \"molecular-biology_promoters\"), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a benchmark comparison of 10 classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this initial analysis, we will apply 10 different classification algorithms to each dataset. Initially, we will use Monte Carlo cross validation (with *no* hyperparameter optimization). To keep the execution time reasonable, we will do 5 iterations of Monte Carlo cross validation.\n",
    "\n",
    "ShinyLearner is executed within a Docker container. The ShinyLearner [web application](http://bioapps.byu.edu/shinylearner/) enables us to more easily build commands for executing ShinyLearner within Docker at the command line. We used this tool to create a template command. Below we modify that template and execute ShinyLearner for each dataset. We also indicate that we want to one-hot encode (`--ohe`) and scale the data (`--scale`) and that we want to impute any missing values (`--impute`).\n",
    "\n",
    "(*This process takes awhile to execute. You won't see any output until the analysis has completed. To facilitate this long-running execution, you could [run this notebook at the command line](https://stackoverflow.com/a/40311709). Also, we could use the `shinylearner_gpu` Docker image to speed up the keras algorithm, but that requires `nvidia-docker` to be installed, so we are using the regular, non-GPU image.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "function runShinyLearner {\n",
    "  dataset_file_path=\"$1\"  \n",
    "  dataset_file_name=\"$(basename $dataset_file_path)\"\n",
    "  dataset_name=\"${dataset_file_name/\\.tsv/}\"\n",
    "  dataset_results_dir_path=\"$(pwd)/Results_Basic/$dataset_name\"\n",
    "  \n",
    "  mkdir -p \"$dataset_results_dir_path\"\n",
    "\n",
    "  docker run --rm \\\n",
    "    -v \"$(pwd)/Datasets\":/InputData \\\n",
    "    -v \"$dataset_results_dir_path\":/OutputData \\\n",
    "    --user $(id -u):$(id -g) \\\n",
    "    srp33/shinylearner:version513 \\\n",
    "    /UserScripts/classification_montecarlo \\\n",
    "      --data \"$dataset_file_name\" \\\n",
    "      --description \"$dataset_name\" \\\n",
    "      --iterations 5 \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/keras/dnn/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/mlr/xgboost/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/mlr/h2o.randomForest/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/mlr/mlp/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/sklearn/decision_tree/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/sklearn/logistic_regression/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/sklearn/svm/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/arff/weka/HoeffdingTree/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/arff/weka/MultilayerPerceptron/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/arff/weka/SimpleLogistic/default*\" \\\n",
    "      --output-dir \"/OutputData\" \\\n",
    "      --ohe false \\\n",
    "      --scale robust \\\n",
    "      --impute true \\\n",
    "      --verbose false\n",
    "}\n",
    "\n",
    "rm -rf Results_Basic\n",
    "\n",
    "for dataset_file_path in ./Datasets/*.tsv\n",
    "do\n",
    "  runShinyLearner \"$dataset_file_path\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating the benchmark comparison with hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShinyLearner provides an option to optimize a classification algorithm's hyperparameters. To accomplish this, it uses nested cross validation. This process requires more computational time, but it often increases classification accuracy. In the code below, we execute the same 10 classification algorithms on the same 10 datasets. There are some differences in the code below compared to the code above:\n",
    "\n",
    "1. We store the output in `Results_ParamsOptimized` rather than `Results_Basic`.\n",
    "2. We use the `nestedclassification_montecarlo` user script rather than `classification_montecarlo`.\n",
    "3. The path specified for each classification algorithm ends with `*` rather than `default*`. This tells ShinyLearner to evaluate all hyperparameter combinations, not just default ones.\n",
    "4. We indicate that we want to use 5 \"outer\" iterations and 3 \"inner\" iterations (to optimize hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "function runShinyLearner {\n",
    "  dataset_file_path=\"$1\"\n",
    "  dataset_file_name=\"$(basename $dataset_file_path)\"\n",
    "  dataset_name=\"${dataset_file_name/\\.tsv/}\"\n",
    "  dataset_results_dir_path=\"$(pwd)/Results_ParamsOptimized/$dataset_name\"\n",
    "  \n",
    "  mkdir -p $dataset_results_dir_path\n",
    "\n",
    "  docker run --rm \\\n",
    "    -v \"$(pwd)/Datasets\":/InputData \\\n",
    "    -v \"$dataset_results_dir_path\":/OutputData \\\n",
    "    --user $(id -u):$(id -g) \\\n",
    "    srp33/shinylearner:version513 \\\n",
    "    /UserScripts/nestedclassification_montecarlo \\\n",
    "      --data \"$dataset_file_name\" \\\n",
    "      --description \"$dataset_name\" \\\n",
    "      --outer-iterations 5 \\\n",
    "      --inner-iterations 3 \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/keras/dnn/*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/mlr/xgboost/*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/mlr/h2o.randomForest/*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/mlr/mlp/*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/sklearn/decision_tree/*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/sklearn/logistic_regression/*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/sklearn/svm/*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/arff/weka/HoeffdingTree/*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/arff/weka/MultilayerPerceptron/*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/arff/weka/SimpleLogistic/*\" \\\n",
    "      --ohe false \\\n",
    "      --scale robust \\\n",
    "      --impute true \\\n",
    "      --verbose false\n",
    "}\n",
    "\n",
    "rm -rf Results_ParamsOptimized\n",
    "\n",
    "for dataset_file_path in ./Datasets/*.tsv\n",
    "do\n",
    "  runShinyLearner \"$dataset_file_path\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating the benchmark comparison with feature selection (along with classification)\n",
    "\n",
    "In this example, we will try 5 feature-selection algorithms in combination with the same 10 classification algorithms that we used previously. Although we could optimize hyperparameters as well, we won't do that, to reduce computational complexity. We have changed the following from the previous example:\n",
    "\n",
    "* We store the results in the `Results_FeatureSelection` directory.\n",
    "* We use the `nestedboth_montecarlo` user script.\n",
    "* We use default hyperparameters.\n",
    "* We added `--fs-algo` and `--num-features` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "function runShinyLearner {\n",
    "  dataset_file_path=\"$1\"\n",
    "  dataset_file_name=\"$(basename $dataset_file_path)\"\n",
    "  dataset_name=\"${dataset_file_name/\\.tsv/}\"\n",
    "  dataset_results_dir_path=\"$(pwd)/Results_FeatureSelection/$dataset_name\"\n",
    "  \n",
    "  mkdir -p $dataset_results_dir_path\n",
    "\n",
    "  docker run --rm \\\n",
    "    -v \"$(pwd)/Datasets\":/InputData \\\n",
    "    -v \"$dataset_results_dir_path\":/OutputData \\\n",
    "    --user $(id -u):$(id -g) \\\n",
    "    srp33/shinylearner:version513 \\\n",
    "    /UserScripts/nestedboth_montecarlo \\\n",
    "      --data \"$dataset_file_name\" \\\n",
    "      --description \"$dataset_name\" \\\n",
    "      --outer-iterations 5 \\\n",
    "      --inner-iterations 3 \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/keras/dnn/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/mlr/xgboost/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/mlr/h2o.randomForest/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/mlr/mlp/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/sklearn/decision_tree/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/sklearn/logistic_regression/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/tsv/sklearn/svm/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/arff/weka/HoeffdingTree/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/arff/weka/MultilayerPerceptron/default*\" \\\n",
    "      --classif-algo \"/AlgorithmScripts/Classification/arff/weka/SimpleLogistic/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/tsv/mlr/kruskal.test/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/tsv/mlr/randomForestSRC.rfsrc/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/tsv/sklearn/mutual_info/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/tsv/sklearn/random_forest_rfe/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/tsv/sklearn/svm_rfe/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/arff/weka/Correlation/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/arff/weka/GainRatio/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/arff/weka/OneR/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/arff/weka/ReliefF/default*\" \\\n",
    "      --fs-algo \"/AlgorithmScripts/FeatureSelection/arff/weka/SymmetricalUncertainty/default*\" \\\n",
    "      --num-features \"1,3,5,10,15,20,50,200\" \\\n",
    "      --ohe false \\\n",
    "      --scale robust \\\n",
    "      --impute true \\\n",
    "      --verbose false\n",
    "}\n",
    "\n",
    "rm -rf Results_FeatureSelection\n",
    "\n",
    "for dataset_file_path in ./Datasets/*.tsv\n",
    "do\n",
    "  runShinyLearner \"$dataset_file_path\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress output files and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# These files are relatively large and we won't use them to make graphs, so let's delete them.\n",
    "rm -fv Results_ParamsOptimized/*/Nested_ElapsedTime.tsv\n",
    "rm -fv Results_ParamsOptimized/*/Nested_Best.tsv\n",
    "mv Results_ParamsOptimized/diabetes/Nested_Predictions.tsv Results_ParamsOptimized/diabetes/Nested_Predictions.tsv.tmp\n",
    "rm -fv Results_ParamsOptimized/*/Nested_Predictions.tsv\n",
    "mv Results_ParamsOptimized/diabetes/Nested_Predictions.tsv.tmp Results_ParamsOptimized/diabetes/Nested_Predictions.tsv\n",
    "rm -fv Results_FeatureSelection/*/Nested_Predictions.tsv\n",
    "rm -fv Results_FeatureSelection/*/Nested_*ElapsedTime.tsv\n",
    "rm -fv Results_FeatureSelection/*/Nested_Best.tsv\n",
    "\n",
    "rm -rfv Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing and visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the document called `Analyze_Results.Rmd`, which contains R code for analyzing and visualizing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
