# Metrics in ShinyLearner

## Background

* [General Overview of Metrics](https://en.wikipedia.org/wiki/Confusion_matrix)
* [Comparing Classifiers](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers)

## ShinyLearner Metrics

* [AUROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) Area under the ROC curve (the most widely used metric)
* [Accuracy](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) Accuracy
* [BalancedAccuracy](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) Balanced Accuracy
* [Brier](https://en.wikipedia.org/wiki/Brier_score) Brier Score
* [F1](https://en.wikipedia.org/wiki/F1_score) F1 Score
* [FDR](https://en.wikipedia.org/wiki/False_discovery_rate) False discovery rate
* [FNR](https://en.wikipedia.org/wiki/False_positives_and_false_negatives) False negative rate
* [FPR](https://en.wikipedia.org/wiki/False_positives_and_false_negatives) False positive rate
* [MCC](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) Matthews correlation coefficient
* [MMCE](http://stats.stackexchange.com/questions/149553/how-to-interpret-concretely-the-misclassification-error) Mean misclassification error
* [NPV](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values) Negative predictive value
* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall) (Positive predictive value)
* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall) (sensitivity)
* [TNR](https://en.wikipedia.org/wiki/Precision_and_recall) True negative rate (specificity)
* [TPR](https://en.wikipedia.org/wiki/Precision_and_recall) True positive rate (sensitivity)

## Additional information

Descriptions from [*mlr*](https://mlr-org.github.io/mlr/) package
